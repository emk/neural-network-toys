//! This is a combination of the tutorial from
//! https://visualstudiomagazine.com/articles/2014/08/01/batch-training.aspx and
//! a lot of code generated by Copilot, which is apparently _very_ enthusiastic
//! about writing large stretches of neural network code at a go.
//!
//! I ended up needing to check everything Copilot wrote. It did an unbelievable
//! amount of work, but it also made plenty of mistakes. And the entire point of
//! this exercise was for me to internalize the details that lives down below
//! Tensorflow and PyTorch.
//!
//! Thanks to the careful test suites on the `Layer` implementations, this
//! code actually works. I'm particularly happy about the numerical gradient
//! checking, which is a great way to verify that the backpropagation code
//! is vaguely trustworthy.

use std::path::Path;

use anyhow::Result;
use clap::{Parser, Subcommand};
use mnist::MnistBuilder;
use ndarray::Array2;
use training::TrainAndTestData;

mod csv_data;
mod history;
mod initialization;
mod layers;
mod network;
mod optimizers;
mod plot;
mod reshape;
mod signals;
mod training;
mod ui;

use crate::{
    csv_data::{read_csv_data, split_data},
    layers::ActivationFunction,
    network::Network,
    training::{train, TrainOpt},
};

#[derive(Debug, Parser)]
#[structopt(about = "Neural network experiments")]
struct Opt {
    #[command(subcommand)]
    cmd: Command,
}

#[derive(Debug, Subcommand)]
enum Command {
    /// Train a neural network on the iris dataset.
    Iris(IrisOpt),

    /// Train a neural network on the MNIST dataset.
    Mnist(MnistOpt),
}

#[derive(Debug, Parser)]
struct IrisOpt {
    /// Our training options.
    #[structopt(flatten)]
    train: TrainOpt,

    /// Training/test split.
    #[arg(long = "split", default_value = "0.8")]
    split: f32,

    /// Hidden layer size. This needs to be adjusted if you change the dropout.
    #[arg(long = "hidden-layer-width", default_value = "14")]
    hidden_layer_width: usize,
}

#[derive(Debug, Parser)]
struct MnistOpt {
    /// Our training options.
    #[structopt(flatten)]
    train: TrainOpt,

    /// Hidden layer size. This needs to be adjusted if you change the dropout.
    #[arg(long = "hidden-layer-width", default_value = "128")]
    hidden_layer_width: usize,

    /// Hidden layer count.
    #[arg(long = "hidden-layers", default_value = "2")]
    hidden_layers: usize,
}

/// Our main entry point.
fn main() -> Result<()> {
    env_logger::init();
    let opt = Opt::parse();

    match opt.cmd {
        Command::Iris(opt) => train_iris(opt)?,
        Command::Mnist(opt) => train_mnist(opt)?,
    }
    Ok(())
}

/// Read the iris data from a CSV file and train a neural network on it.
fn train_iris(opt: IrisOpt) -> Result<()> {
    let feature_count = 4;
    let class_count = 3;

    let (inputs, targets) = read_csv_data(Path::new("data/iris.csv"), feature_count)?;
    let train_and_test_data = split_data(inputs, targets, opt.split);
    eprintln!(
        "Training data: {} examples, test data: {} examples",
        train_and_test_data.train_inputs.nrows(),
        train_and_test_data.test_inputs.nrows()
    );

    let mut network = Network::new(feature_count);
    let activation = opt.train.activation();
    network.add_fully_connected_layer(opt.hidden_layer_width, activation);
    network.add_dropout_layer(1.0 - opt.train.dropout);
    network.add_fully_connected_layer(class_count, ActivationFunction::Softmax);

    train("iris", opt.train, &mut network, &train_and_test_data)
}

/// Read the MNIST data from a file using the `minist` crate and train a neural
/// network on it.
fn train_mnist(opt: MnistOpt) -> Result<()> {
    let mnist = MnistBuilder::new()
        // If you omit the trailing "/" here, the downloader breaks.
        .base_path("data/mnist/")
        .label_format_one_hot()
        .training_set_length(50000)
        .validation_set_length(10000)
        .test_set_length(10000)
        .download_and_extract()
        .finalize();

    // Convert a giant array of u8s into a 2D array of f32s, each with dimension
    // `[examples, features]`.
    let array2_f32_from_vec_u8 = |v: Vec<u8>, cols: usize| {
        let v = v.iter().map(|&x| x as f32).collect::<Vec<_>>();
        let rows = v.len() / cols;
        assert!(rows * cols == v.len());
        Array2::from_shape_vec((rows, cols), v).expect("invalid shape")
    };

    // Convert the MNIST data into normalized `[examples, features]` arrays.
    let img_size = 28 * 28;
    let num_digits = 10;
    let train_and_test_data = TrainAndTestData {
        train_inputs: array2_f32_from_vec_u8(mnist.trn_img, img_size) / 255.0,
        train_targets: array2_f32_from_vec_u8(mnist.trn_lbl, num_digits),
        test_inputs: array2_f32_from_vec_u8(mnist.tst_img, img_size) / 255.0,
        test_targets: array2_f32_from_vec_u8(mnist.tst_lbl, num_digits),
    };

    let mut network = Network::new(img_size);
    let activation = opt.train.activation();
    network.add_fully_connected_layer(opt.hidden_layer_width, activation);
    for _ in 1..opt.hidden_layers {
        network.add_fully_connected_layer(opt.hidden_layer_width, activation);
        network.add_dropout_layer(1.0 - opt.train.dropout);
    }
    network.add_fully_connected_layer(num_digits, ActivationFunction::Softmax);

    train("mnist", opt.train, &mut network, &train_and_test_data)
}
