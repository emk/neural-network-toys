/// This is a combination of the tutorial from
/// https://visualstudiomagazine.com/articles/2014/08/01/batch-training.aspx and
/// a lot of code generated by Copilot, which is apparently _very_ enthusiastic
/// about writing large stretches of neural network code at a go.
///
/// I ended up needing to check everything Copilot wrote. It did an unbelievable
/// amount of work, but it also made plenty of mistakes. And the entire point of
/// this exercise was for me to internalize the details that lives down below
/// Tensorflow and PyTorch.
///
/// Thanks to the careful test suites on the `Layer` implementations, this
/// code actually works. I'm particularly happy about the numerical gradient
/// checking, which is a great way to verify that the backpropagation code
/// is vaguely trustworthy.
use std::path::{Path, PathBuf};

use anyhow::Result;
use clap::{Parser, Subcommand};
use csv;
use log::debug;
use ndarray::Array1;
use rand::seq::SliceRandom;

mod layers;

use crate::layers::{Network, SoftmaxLayer, TanhLayer};

#[derive(Debug, Parser)]
#[structopt(
    about = "Neural network experiments"
)]
struct Opt {
    #[command(subcommand)]
    cmd: Command,
}

#[derive(Debug, Subcommand)]
enum Command {
    /// Train a neural network.
    Train(TrainOpt),
}

#[derive(Debug, Parser)]
struct TrainOpt {
    // Parameters are number of epochs, the training/test split, the learning rate, and size of input (4), hidden (7) and output (3) layers.
    /// Number of epochs.
    #[arg(long = "epochs", default_value = "2000")]
    epochs: usize,

    /// Training/test split.
    #[arg(long = "split", default_value = "0.8")]
    split: f64,

    /// Learning rate.
    #[arg(long = "learning-rate", default_value = "0.01")]
    learning_rate: f64,

    /// Input layer size.
    #[arg(long = "input-size", default_value = "4")]
    input_size: usize,

    /// Hidden layer size.
    #[arg(long = "hidden-size", default_value = "7")]
    hidden_size: usize,

    /// Output layer size.
    #[arg(long = "output-size", default_value = "3")]
    output_size: usize,

    /// Path to the data file.
    data: PathBuf,

    /// Path to the output model file.
    model: PathBuf,
}

fn main() -> Result<()> {
    env_logger::init();
    let opt = Opt::parse();

    match opt.cmd {
        Command::Train(opt) => train(opt)?,
    }
    Ok(())
}

/// First attempt at a training function, written largely by Copilot.
fn train(opt: TrainOpt) -> Result<()> {
    let data = read_data(&opt.data)?;
    let (train, test) = split_data(&data, opt.split);
    eprintln!(
        "Training data: {} examples, test data: {} examples",
        train.len(),
        test.len()
    );

    let mut network = Network::new(TanhLayer::new(opt.input_size, opt.hidden_size));
    network.add_layer(SoftmaxLayer::new(opt.hidden_size, opt.output_size));

    for epoch in 0..opt.epochs {
        debug!("Model: {:#?}", network);
        let mut train_loss = 0.0;
        let mut train_correct = 0;
        for (input, target) in &train {
            let loss = network.update(input, target, opt.learning_rate);
            assert!(loss.is_finite(), "loss is not finite: {}", loss);
            let output = network.forward(input);
            train_loss += loss;
            if predicted_class_index(&output) == predicted_class_index(target) {
                train_correct += 1;
            }
        }
        train_loss /= train.len() as f64;

        let mut test_loss = 0.0;
        let mut test_correct = 0;
        for (input, target) in &test {
            let output = network.forward(input);
            let loss = network.loss(&output, target);
            test_loss += loss;
            if predicted_class_index(&output) == predicted_class_index(target) {
                test_correct += 1;
            }
        }
        test_loss /= test.len() as f64;

        eprintln!(
            "Epoch {}: train loss = {:.4} ({}/{}), test loss = {:.4} ({} / {})",
            epoch,
            train_loss,
            train_correct,
            train.len(),
            test_loss,
            test_correct,
            test.len(),
        );
    }

    // TODO: Reimplement `Network::save`.

    Ok(())
}

/// Read our training and test data from a CSV file.
///
/// The file is in the format:
///
/// ```csv
/// sepal_length,sepal_width,petal_length,petal_width,iris_virginica,iris_versicolor,iris_setosa
/// 5.1,3.5,1.4,0.2,0,0,1
/// 4.9,3.0,1.4,0.2,0,0,1
/// ```
///
/// ...where the first four columns are the input data, and the last three are
/// the target data.
fn read_data(path: &Path) -> Result<Vec<(Array1<f64>, Array1<f64>)>> {
    let mut rdr = csv::Reader::from_path(path)?;
    let mut data = Vec::new();
    for result in rdr.records() {
        let record = result?;
        let input = Array1::from(
            record
                .iter()
                .take(4)
                .map(|s| s.parse::<f64>().unwrap())
                .collect::<Vec<_>>(),
        );
        let target = Array1::from(
            record
                .iter()
                .skip(4)
                .map(|s| s.parse::<f64>().unwrap())
                .collect::<Vec<_>>(),
        );
        data.push((input, target));
    }
    Ok(data)
}

/// Split the data into training and test sets.
fn split_data(
    data: &[(Array1<f64>, Array1<f64>)],
    split: f64,
) -> (
    Vec<(Array1<f64>, Array1<f64>)>,
    Vec<(Array1<f64>, Array1<f64>)>,
) {
    // Shuffle first.
    let mut data = data.to_vec();
    let mut rng = rand::thread_rng();
    data.shuffle(&mut rng);

    // Then split.
    let split_index = (data.len() as f64 * split) as usize;
    let train = data[..split_index].to_vec();
    let test = data[split_index..].to_vec();
    (train, test)
}

/// Convert a 1-hot encoded array to a class index.
fn predicted_class_index(output: &Array1<f64>) -> usize {
    output
        .iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .unwrap()
        .0
}
