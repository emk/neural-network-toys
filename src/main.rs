/// This is a combination of the tutorial from
/// https://visualstudiomagazine.com/articles/2014/08/01/batch-training.aspx and
/// a lot of code generated by Copilot, which is apparently _very_ enthusiastic
/// about writing large stretches of neural network code at a go.
///
/// I ended up needing to check everything Copilot wrote. It did an unbelievable
/// amount of work, but it also made plenty of mistakes. And the entire point of
/// this exercise was for me to internalize the details that lives down below
/// Tensorflow and PyTorch.
///
/// Thanks to the careful test suites on the `Layer` implementations, this
/// code actually works. I'm particularly happy about the numerical gradient
/// checking, which is a great way to verify that the backpropagation code
/// is vaguely trustworthy.
use std::path::{Path, PathBuf};

use anyhow::Result;
use clap::{Parser, Subcommand};
use csv;
use log::debug;
use ndarray::Array1;
use plotters::{prelude::{ChartBuilder, IntoDrawingArea, PathElement}, style::{WHITE, IntoFont, RED, BLUE, BLACK, Color}, series::LineSeries};
use plotters_svg::SVGBackend;
use rand::seq::SliceRandom;

mod layers;

use crate::layers::{DropoutLayer, Network, SoftmaxLayer, TanhLayer};

#[derive(Debug, Parser)]
#[structopt(
    about = "Neural network experiments"
)]
struct Opt {
    #[command(subcommand)]
    cmd: Command,
}

#[derive(Debug, Subcommand)]
enum Command {
    /// Train a neural network.
    Train(TrainOpt),
}

#[derive(Debug, Parser)]
struct TrainOpt {
    // Parameters are number of epochs, the training/test split, the learning rate, and size of input (4), hidden (7) and output (3) layers.
    /// Number of epochs.
    #[arg(long = "epochs", default_value = "2000")]
    epochs: usize,

    /// Training/test split.
    #[arg(long = "split", default_value = "0.8")]
    split: f64,

    /// Learning rate.
    #[arg(long = "learning-rate", default_value = "0.01")]
    learning_rate: f64,

    #[arg(long = "dropout", default_value = "0.5")]
    dropout: f64,

    /// Input layer size.
    #[arg(long = "input-size", default_value = "4")]
    input_size: usize,

    /// Hidden layer size. This needs to be adjusted if you change the dropout.
    #[arg(long = "hidden-size", default_value = "14")]
    hidden_size: usize,

    /// Output layer size.
    #[arg(long = "output-size", default_value = "3")]
    output_size: usize,

    /// Path to save plot of training and test loss.
    #[arg(long = "plot")]
    plot: Option<PathBuf>,

    /// Path to the data file.
    data: PathBuf,

    /// Path to the output model file.
    model: PathBuf,
}

fn main() -> Result<()> {
    env_logger::init();
    let opt = Opt::parse();

    match opt.cmd {
        Command::Train(opt) => train(opt)?,
    }
    Ok(())
}

/// First attempt at a training function, written largely by Copilot.
fn train(opt: TrainOpt) -> Result<()> {
    let data = read_data(&opt.data)?;
    let (mut train, test) = split_data(&data, opt.split);
    eprintln!(
        "Training data: {} examples, test data: {} examples",
        train.len(),
        test.len()
    );

    let mut network = Network::new(TanhLayer::new(opt.input_size, opt.hidden_size));
    network.add_layer(DropoutLayer::new(opt.hidden_size, 1.0-opt.dropout));
    network.add_layer(SoftmaxLayer::new(opt.hidden_size, opt.output_size));

    let mut epoch_training_losses = Vec::new();
    let mut epoch_test_losses = Vec::new();

    let mut rng = rand::thread_rng();
    for epoch in 0..opt.epochs {
        debug!("Model: {:#?}", network);
        let mut train_loss = 0.0;
        let mut train_correct = 0;

        train.shuffle(&mut rng);
        for (input, target) in &train {
            network.update(input, target, opt.learning_rate);
            let output = network.forward(input);
            let loss = network.loss(&output, target);
            assert!(loss.is_finite(), "loss is not finite: {}", loss);
            train_loss += loss;
            if predicted_class_index(&output) == predicted_class_index(target) {
                train_correct += 1;
            }
        }
        train_loss /= train.len() as f64;

        let mut test_loss = 0.0;
        let mut test_correct = 0;
        for (input, target) in &test {
            let output = network.forward(input);
            let loss = network.loss(&output, target);
            test_loss += loss;
            if predicted_class_index(&output) == predicted_class_index(target) {
                test_correct += 1;
            }
        }
        test_loss /= test.len() as f64;

        epoch_training_losses.push(train_loss);
        epoch_test_losses.push(test_loss);

        eprintln!(
            "Epoch {}: train loss = {:.4} ({}/{}), test loss = {:.4} ({} / {})",
            epoch,
            train_loss,
            train_correct,
            train.len(),
            test_loss,
            test_correct,
            test.len(),
        );
    }

    // Optionally plot the training and test loss.
    if let Some(plot_path) = opt.plot {
        plot_loss(&plot_path, &epoch_training_losses, &epoch_test_losses)?;
    }

    // TODO: Reimplement `Network::save`.

    Ok(())
}

/// Read our training and test data from a CSV file.
///
/// The file is in the format:
///
/// ```csv
/// sepal_length,sepal_width,petal_length,petal_width,iris_virginica,iris_versicolor,iris_setosa
/// 5.1,3.5,1.4,0.2,0,0,1
/// 4.9,3.0,1.4,0.2,0,0,1
/// ```
///
/// ...where the first four columns are the input data, and the last three are
/// the target data.
fn read_data(path: &Path) -> Result<Vec<(Array1<f64>, Array1<f64>)>> {
    let mut rdr = csv::Reader::from_path(path)?;
    let mut data = Vec::new();
    for result in rdr.records() {
        let record = result?;
        let input = Array1::from(
            record
                .iter()
                .take(4)
                .map(|s| s.parse::<f64>().unwrap())
                .collect::<Vec<_>>(),
        );
        let target = Array1::from(
            record
                .iter()
                .skip(4)
                .map(|s| s.parse::<f64>().unwrap())
                .collect::<Vec<_>>(),
        );
        data.push((input, target));
    }
    Ok(data)
}

/// Split the data into training and test sets.
fn split_data(
    data: &[(Array1<f64>, Array1<f64>)],
    split: f64,
) -> (
    Vec<(Array1<f64>, Array1<f64>)>,
    Vec<(Array1<f64>, Array1<f64>)>,
) {
    // Shuffle first.
    let mut data = data.to_vec();
    let mut rng = rand::thread_rng();
    data.shuffle(&mut rng);

    // Then split.
    let split_index = (data.len() as f64 * split) as usize;
    let train = data[..split_index].to_vec();
    let test = data[split_index..].to_vec();
    (train, test)
}

/// Convert a 1-hot encoded array to a class index.
fn predicted_class_index(output: &Array1<f64>) -> usize {
    output
        .iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .unwrap()
        .0
}

/// Use `plotters` to plot the training and test losses for each epoch as an
/// SVG and save it to `path`. Almost entirely written by Copilot.
fn plot_loss(
    path: &Path,
    training_losses: &[f64],
    test_losses: &[f64],
) -> Result<()> {
    let root = SVGBackend::new(path, (640, 480)).into_drawing_area();
    root.fill(&WHITE)?;

    let mut chart = ChartBuilder::on(&root)
        .caption("Loss", ("sans-serif", 50).into_font())
        .margin(5)
        .x_label_area_size(30)
        .y_label_area_size(40)
        .build_cartesian_2d(0.0f64..training_losses.len() as f64, 0.0f64..1.0f64)?;

    chart
        .configure_mesh()
        .disable_x_mesh()
        .disable_y_mesh()
        .x_desc("Epoch")
        .y_desc("Loss")
        .draw()?;

    chart
        .draw_series(LineSeries::new(
            training_losses.iter().enumerate().map(|(x, y)| (x as f64, *y)),
            &RED,
        ))?
        .label("Training")
        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));

    chart
        .draw_series(LineSeries::new(
            test_losses.iter().enumerate().map(|(x, y)| (x as f64, *y)),
            &BLUE,
        ))?
        .label("Test")
        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));

    chart
        .configure_series_labels()
        .border_style(&BLACK)
        .background_style(&WHITE.mix(0.8))
        .draw()?;

    Ok(())
}
